{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f48546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple/, https://aws:****@quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/torch/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch>=2.0.0 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.7.1-cp311-none-macosx_11_0_arm64.whl.metadata (29 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers>=4.30.0 (from -r requirements.txt (line 2))\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/datasets/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets>=2.12.0 (from -r requirements.txt (line 3))\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/sentence-transformers/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sentence-transformers>=2.2.0 (from -r requirements.txt (line 4))\n",
      "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/scikit-learn/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scikit-learn>=1.3.0 (from -r requirements.txt (line 5))\n",
      "  Downloading scikit_learn-1.7.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/numpy/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy>=1.24.0 (from -r requirements.txt (line 6))\n",
      "  Downloading numpy-2.3.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/tqdm/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tqdm>=4.65.0 (from -r requirements.txt (line 7))\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/requests/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting requests>=2.31.0 (from -r requirements.txt (line 8))\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/filelock/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting filelock (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/sathiskumarparanthaman/.local/lib/python3.11/site-packages (from torch>=2.0.0->-r requirements.txt (line 1)) (4.14.1)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/sympy/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting sympy>=1.13.3 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/networkx/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting networkx (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/jinja2/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting jinja2 (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/fsspec/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting fsspec (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/huggingface-hub/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting huggingface-hub<1.0,>=0.30.0 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sathiskumarparanthaman/.local/lib/python3.11/site-packages (from transformers>=4.30.0->-r requirements.txt (line 2)) (25.0)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/pyyaml/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyyaml>=5.1 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/regex/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting regex!=2019.12.17 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/tokenizers/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tokenizers<0.22,>=0.21 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/safetensors/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting safetensors>=0.4.3 (from transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/hf-xet/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.30.0->transformers>=4.30.0->-r requirements.txt (line 2))\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl.metadata (879 bytes)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/pyarrow/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pyarrow>=15.0.0 (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/dill/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting dill<0.3.9,>=0.3.0 (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/pandas/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pandas (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/xxhash/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting xxhash (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/multiprocess/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting multiprocess<0.70.17 (from datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec (from torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/aiohttp/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading aiohttp-3.12.14-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/scipy/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting scipy (from sentence-transformers>=2.2.0->-r requirements.txt (line 4))\n",
      "  Downloading scipy-1.16.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/pillow/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting Pillow (from sentence-transformers>=2.2.0->-r requirements.txt (line 4))\n",
      "  Downloading pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/joblib/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting joblib>=1.2.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 5))\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/threadpoolctl/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting threadpoolctl>=3.1.0 (from scikit-learn>=1.3.0->-r requirements.txt (line 5))\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/charset-normalizer/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting charset_normalizer<4,>=2 (from requests>=2.31.0->-r requirements.txt (line 8))\n",
      "  Downloading charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/idna/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting idna<4,>=2.5 (from requests>=2.31.0->-r requirements.txt (line 8))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/urllib3/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting urllib3<3,>=1.21.1 (from requests>=2.31.0->-r requirements.txt (line 8))\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/certifi/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting certifi>=2017.4.17 (from requests>=2.31.0->-r requirements.txt (line 8))\n",
      "  Downloading certifi-2025.7.9-py3-none-any.whl.metadata (2.4 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/aiohappyeyeballs/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/aiosignal/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/attrs/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/frozenlist/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/multidict/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading multidict-6.6.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/propcache/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/yarl/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Downloading yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (73 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/mpmath/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/markupsafe/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->-r requirements.txt (line 1))\n",
      "  Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sathiskumarparanthaman/.local/lib/python3.11/site-packages (from pandas->datasets>=2.12.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/pytz/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting pytz>=2020.1 (from pandas->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "\u001b[33mWARNING: 401 Error, Credentials not correct for https://quickstep-905418116080.d.codeartifact.eu-central-1.amazonaws.com/pypi/librarian/simple/tzdata/\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tzdata>=2022.7 (from pandas->datasets>=2.12.0->-r requirements.txt (line 3))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/sathiskumarparanthaman/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.12.0->-r requirements.txt (line 3)) (1.17.0)\n",
      "Downloading torch-2.7.1-cp311-none-macosx_11_0_arm64.whl (68.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Downloading scikit_learn-1.7.0-cp311-cp311-macosx_12_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.1-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp311-cp311-macosx_10_9_universal2.whl (198 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading aiohttp-3.12.14-cp311-cp311-macosx_11_0_arm64.whl (470 kB)\n",
      "Downloading multidict-6.6.3-cp311-cp311-macosx_11_0_arm64.whl (44 kB)\n",
      "Downloading yarl-1.20.1-cp311-cp311-macosx_11_0_arm64.whl (89 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2025.7.9-py3-none-any.whl (159 kB)\n",
      "Downloading frozenlist-1.7.0-cp311-cp311-macosx_11_0_arm64.whl (47 kB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading propcache-0.3.2-cp311-cp311-macosx_11_0_arm64.whl (43 kB)\n",
      "Downloading pyarrow-20.0.0-cp311-cp311-macosx_12_0_arm64.whl (30.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.9/30.9 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-macosx_11_0_arm64.whl (172 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading scipy-1.16.0-cp311-cp311-macosx_14_0_arm64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading pillow-11.3.0-cp311-cp311-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, pyarrow, propcache, Pillow, numpy, networkx, multidict, MarkupSafe, joblib, idna, hf-xet, fsspec, frozenlist, filelock, dill, charset_normalizer, certifi, attrs, aiohappyeyeballs, yarl, scipy, requests, pandas, multiprocess, jinja2, aiosignal, torch, scikit-learn, huggingface-hub, aiohttp, tokenizers, transformers, datasets, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/44\u001b[0m [sentence-transformers]sformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 Pillow-11.3.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.14 aiosignal-1.4.0 attrs-25.3.0 certifi-2025.7.9 charset_normalizer-3.4.2 datasets-4.0.0 dill-0.3.8 filelock-3.18.0 frozenlist-1.7.0 fsspec-2025.3.0 hf-xet-1.1.5 huggingface-hub-0.33.4 idna-3.10 jinja2-3.1.6 joblib-1.5.1 mpmath-1.3.0 multidict-6.6.3 multiprocess-0.70.16 networkx-3.5 numpy-2.3.1 pandas-2.3.1 propcache-0.3.2 pyarrow-20.0.0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 scikit-learn-1.7.0 scipy-1.16.0 sentence-transformers-5.0.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.2 torch-2.7.1 tqdm-4.67.1 transformers-4.53.2 tzdata-2025.2 urllib3-2.5.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "195553bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import platform\n",
    "import requests\n",
    "import gzip\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "import pickle\n",
    "import warnings\n",
    "from torch.serialization import safe_globals\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dd9a1f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26b53591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmbeddingConfig:\n",
    "    \"\"\"Configuration for multilingual embedding model\"\"\"\n",
    "    base_model: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    embedding_dim: int = 768  # Good dimension for multilingual tasks\n",
    "    max_length: int = 512\n",
    "    batch_size: int = 16  # Optimized for Mac M1/M2 and Windows\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 4\n",
    "    warmup_steps: int = 1000\n",
    "    temperature: float = 0.05\n",
    "    margin: float = 0.3\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    max_grad_norm: float = 1.0\n",
    "    weight_decay: float = 0.01\n",
    "    save_steps: int = 1000\n",
    "    eval_steps: int = 500\n",
    "    # Dataset configuration\n",
    "    dataset_name: str = \"sentence-transformers/all-nli\"\n",
    "    max_train_samples: int = 50000\n",
    "    max_val_samples: int = 5000\n",
    "    dataset_config_name: str = \"pair-class\"\n",
    "    # Multilingual settings\n",
    "    languages: List[str] = None  # Will be set to common languages\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.languages is None:\n",
    "            self.languages = ['en', 'de', 'fr', 'es', 'it', 'pt', 'nl', 'pl', 'ru', 'zh', 'ja', 'ar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3060ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualDatasetProcessor:\n",
    "    \"\"\"Process datasets for multilingual embedding training\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        self.config = config\n",
    "        self.data_dir = \"embedding_data\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        \n",
    "    def download_and_process_allnli(self):\n",
    "        \"\"\"Download and process AllNLI dataset\"\"\"\n",
    "        logger.info(\"Loading AllNLI dataset...\")\n",
    "        \n",
    "        try:\n",
    "            # Load the dataset\n",
    "            dataset = load_dataset(self.config.dataset_name, self.config.dataset_config_name)\n",
    "            print(f\"Dataset loaded: {self.config.dataset_name}\", dataset)\n",
    "            # Process training data\n",
    "            train_data = self._process_nli_split(dataset['train'], 'train')\n",
    "            val_data = self._process_nli_split(dataset['dev'], 'validation')\n",
    "            \n",
    "            # Save processed data\n",
    "            self._save_processed_data(train_data, 'train_data.json')\n",
    "            self._save_processed_data(val_data, 'val_data.json')\n",
    "            \n",
    "            logger.info(f\"Processed {len(train_data)} training samples and {len(val_data)} validation samples\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing AllNLI dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def download_and_process_msmarco(self):\n",
    "        \"\"\"Download and process MS MARCO dataset\"\"\"\n",
    "        logger.info(\"Loading MS MARCO dataset...\")\n",
    "        \n",
    "        try:\n",
    "            # Load MS MARCO passage ranking dataset\n",
    "            dataset = load_dataset(\"ms_marco\", \"v1.1\")\n",
    "            \n",
    "            # Process the dataset\n",
    "            train_data = self._process_msmarco_split(dataset['train'])\n",
    "            val_data = self._process_msmarco_split(dataset['validation'])\n",
    "            \n",
    "            # Save processed data\n",
    "            self._save_processed_data(train_data, 'train_data.json')\n",
    "            self._save_processed_data(val_data, 'val_data.json')\n",
    "            \n",
    "            logger.info(f\"Processed {len(train_data)} training samples and {len(val_data)} validation samples\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing MS MARCO dataset: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _process_nli_split(self, split_data, split_name):\n",
    "        \"\"\"Process NLI data split\"\"\"\n",
    "        processed_data = []\n",
    "        max_samples = self.config.max_train_samples if split_name == 'train' else self.config.max_val_samples\n",
    "        \n",
    "        # Sample data if too large\n",
    "        if len(split_data) > max_samples:\n",
    "            indices = random.sample(range(len(split_data)), max_samples)\n",
    "            split_data = split_data.select(indices)\n",
    "        \n",
    "        for item in tqdm(split_data, desc=f\"Processing {split_name} data\"):\n",
    "            if item['label'] == 0:  # Entailment - positive pair\n",
    "                processed_data.append({\n",
    "                    'query': item['premise'],\n",
    "                    'positive': item['hypothesis'],\n",
    "                    'negative': self._get_random_negative(split_data, item['premise'])\n",
    "                })\n",
    "            elif item['label'] == 2:  # Contradiction - can be used as negative\n",
    "                # Find a neutral or entailment pair for positive\n",
    "                positive_text = self._get_random_positive(split_data, item['premise'])\n",
    "                if positive_text:\n",
    "                    processed_data.append({\n",
    "                        'query': item['premise'],\n",
    "                        'positive': positive_text,\n",
    "                        'negative': item['hypothesis']\n",
    "                    })\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def _process_msmarco_split(self, split_data):\n",
    "        \"\"\"Process MS MARCO data split\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for item in tqdm(split_data, desc=\"Processing MS MARCO data\"):\n",
    "            if 'passages' in item and 'is_selected' in item:\n",
    "                query = item['query']\n",
    "                positive_passages = [p['passage_text'] for p, selected in zip(item['passages'], item['is_selected']) if selected]\n",
    "                negative_passages = [p['passage_text'] for p, selected in zip(item['passages'], item['is_selected']) if not selected]\n",
    "                \n",
    "                # Create training pairs\n",
    "                for positive in positive_passages:\n",
    "                    if negative_passages:\n",
    "                        negative = random.choice(negative_passages)\n",
    "                        processed_data.append({\n",
    "                            'query': query,\n",
    "                            'positive': positive,\n",
    "                            'negative': negative\n",
    "                        })\n",
    "        \n",
    "        return processed_data\n",
    "    \n",
    "    def _get_random_negative(self, dataset, query):\n",
    "        \"\"\"Get a random negative example\"\"\"\n",
    "        max_attempts = 10\n",
    "        for _ in range(max_attempts):\n",
    "            random_item = random.choice(dataset)\n",
    "            if random_item['premise'] != query:\n",
    "                return random_item['hypothesis']\n",
    "        return \"This is a random negative example.\"\n",
    "    \n",
    "    def _get_random_positive(self, dataset, query):\n",
    "        \"\"\"Get a random positive example\"\"\"\n",
    "        for item in dataset:\n",
    "            if item['premise'] == query and item['label'] == 0:\n",
    "                return item['hypothesis']\n",
    "        return None\n",
    "    \n",
    "    def _save_processed_data(self, data, filename):\n",
    "        \"\"\"Save processed data to JSON file\"\"\"\n",
    "        filepath = os.path.join(self.data_dir, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        logger.info(f\"Saved {len(data)} samples to {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "699dd1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddingDataset(Dataset):\n",
    "    \"\"\"Dataset for multilingual embedding training\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str, tokenizer, max_length: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.data = self.load_data(data_path)\n",
    "        \n",
    "    def load_data(self, data_path: str) -> List[Dict]:\n",
    "        \"\"\"Load training data from JSON file\"\"\"\n",
    "        if not os.path.exists(data_path):\n",
    "            raise FileNotFoundError(f\"Data file not found: {data_path}\")\n",
    "        \n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(data)} samples from {data_path}\")\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize query, positive, and negative examples\n",
    "        query = self.tokenize_text(item['query'])\n",
    "        positive = self.tokenize_text(item['positive'])\n",
    "        negative = self.tokenize_text(item['negative'])\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'positive': positive,\n",
    "            'negative': negative\n",
    "        }\n",
    "    \n",
    "    def tokenize_text(self, text: str) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Tokenize text with proper padding and truncation\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoded['input_ids'].squeeze(),\n",
    "            'attention_mask': encoded['attention_mask'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "de0c161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddingModel(nn.Module):\n",
    "    \"\"\"Multilingual embedding model with advanced pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: EmbeddingConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.base_model = AutoModel.from_pretrained(config.base_model)\n",
    "        \n",
    "        # Get the hidden size from base model\n",
    "        self.hidden_size = self.base_model.config.hidden_size\n",
    "        \n",
    "        # Projection layers\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, config.embedding_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.embedding_dim, config.embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm = nn.LayerNorm(config.embedding_dim)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize projection layer weights\"\"\"\n",
    "        for module in self.projection:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Mean pooling with attention mask\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    \n",
    "    def cls_pooling(self, model_output):\n",
    "        \"\"\"CLS token pooling\"\"\"\n",
    "        return model_output[0][:, 0]\n",
    "    \n",
    "    def max_pooling(self, model_output, attention_mask):\n",
    "        \"\"\"Max pooling with attention mask\"\"\"\n",
    "        token_embeddings = model_output[0]\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        token_embeddings[input_mask_expanded == 0] = -1e9\n",
    "        return torch.max(token_embeddings, 1)[0]\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, pooling_strategy='mean'):\n",
    "        \"\"\"Forward pass through the model\"\"\"\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Apply different pooling strategies\n",
    "        if pooling_strategy == 'mean':\n",
    "            pooled_output = self.mean_pooling(outputs, attention_mask)\n",
    "        elif pooling_strategy == 'cls':\n",
    "            pooled_output = self.cls_pooling(outputs)\n",
    "        elif pooling_strategy == 'max':\n",
    "            pooled_output = self.max_pooling(outputs, attention_mask)\n",
    "        else:\n",
    "            pooled_output = self.mean_pooling(outputs, attention_mask)\n",
    "        \n",
    "        # Apply projection\n",
    "        embeddings = self.projection(pooled_output)\n",
    "        \n",
    "        # Apply dropout and layer norm\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        \n",
    "        # L2 normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9cbc770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleLoss(nn.Module):\n",
    "    \"\"\"Combined loss function for better training\"\"\"\n",
    "    \n",
    "    def __init__(self, margin: float = 0.3, temperature: float = 0.05, alpha: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def triplet_loss(self, anchor, positive, negative):\n",
    "        \"\"\"Triplet loss\"\"\"\n",
    "        pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
    "        neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
    "        loss = F.relu(pos_dist - neg_dist + self.margin)\n",
    "        return loss.mean()\n",
    "    \n",
    "    def contrastive_loss(self, anchor, positive, negative):\n",
    "        \"\"\"Contrastive loss with temperature scaling\"\"\"\n",
    "        pos_sim = F.cosine_similarity(anchor, positive, dim=1) / self.temperature\n",
    "        neg_sim = F.cosine_similarity(anchor, negative, dim=1) / self.temperature\n",
    "        \n",
    "        logits = torch.cat([pos_sim.unsqueeze(1), neg_sim.unsqueeze(1)], dim=1)\n",
    "        labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n",
    "        \n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"Compute combined loss\"\"\"\n",
    "        triplet_loss = self.triplet_loss(anchor, positive, negative)\n",
    "        contrastive_loss = self.contrastive_loss(anchor, positive, negative)\n",
    "        \n",
    "        return self.alpha * triplet_loss + (1 - self.alpha) * contrastive_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5abb2d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddingTrainer:\n",
    "    \"\"\"Trainer for multilingual embedding models\"\"\"\n",
    "    \n",
    "    def __init__(self, model: MultilingualEmbeddingModel, config: EmbeddingConfig):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        \n",
    "        # Setup device (Mac M1/M2 and Windows compatible)\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')  # Mac M1/M2\n",
    "            logger.info(\"Using MPS (Apple Silicon) device\")\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')  # Windows/Linux with CUDA\n",
    "            logger.info(f\"Using CUDA device: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            logger.info(\"Using CPU device\")\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Initialize loss\n",
    "        self.criterion = MultiScaleLoss(\n",
    "            margin=config.margin,\n",
    "            temperature=config.temperature,\n",
    "            alpha=0.5\n",
    "        )\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config.learning_rate,\n",
    "            weight_decay=config.weight_decay\n",
    "        )\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.global_step = 0\n",
    "        \n",
    "    def train_epoch(self, dataloader: DataLoader, epoch: int):\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{self.config.num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                query = {k: v.to(self.device) for k, v in batch['query'].items()}\n",
    "                positive = {k: v.to(self.device) for k, v in batch['positive'].items()}\n",
    "                negative = {k: v.to(self.device) for k, v in batch['negative'].items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                query_emb = self.model(**query)\n",
    "                positive_emb = self.model(**positive)\n",
    "                negative_emb = self.model(**negative)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.criterion(query_emb, positive_emb, negative_emb)\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.config.gradient_accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "                \n",
    "                total_loss += loss.item() * self.config.gradient_accumulation_steps\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f'{loss.item() * self.config.gradient_accumulation_steps:.4f}',\n",
    "                    'avg_loss': f'{total_loss / num_batches:.4f}'\n",
    "                })\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if self.global_step % self.config.save_steps == 0:\n",
    "                    self.save_checkpoint(f'checkpoint_step_{self.global_step}.pt')\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return total_loss / num_batches if num_batches > 0 else 0\n",
    "    \n",
    "    def validate(self, dataloader: DataLoader):\n",
    "        \"\"\"Validate the model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader, desc='Validating'):\n",
    "                try:\n",
    "                    query = {k: v.to(self.device) for k, v in batch['query'].items()}\n",
    "                    positive = {k: v.to(self.device) for k, v in batch['positive'].items()}\n",
    "                    negative = {k: v.to(self.device) for k, v in batch['negative'].items()}\n",
    "                    \n",
    "                    query_emb = self.model(**query)\n",
    "                    positive_emb = self.model(**positive)\n",
    "                    negative_emb = self.model(**negative)\n",
    "                    \n",
    "                    loss = self.criterion(query_emb, positive_emb, negative_emb)\n",
    "                    total_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        return total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    \n",
    "    def train(self, train_dataloader: DataLoader, val_dataloader: Optional[DataLoader] = None):\n",
    "        \"\"\"Full training loop\"\"\"\n",
    "        # Initialize scheduler\n",
    "        total_steps = len(train_dataloader) * self.config.num_epochs // self.config.gradient_accumulation_steps\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.config.warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        logger.info(f\"Starting training for {self.config.num_epochs} epochs...\")\n",
    "        logger.info(f\"Total training steps: {total_steps}\")\n",
    "        \n",
    "        for epoch in range(self.config.num_epochs):\n",
    "            # Train\n",
    "            train_loss = self.train_epoch(train_dataloader, epoch)\n",
    "            self.train_losses.append(train_loss)\n",
    "            \n",
    "            logger.info(f'Epoch {epoch+1}/{self.config.num_epochs} - Train Loss: {train_loss:.4f}')\n",
    "            \n",
    "            # Validate\n",
    "            if val_dataloader:\n",
    "                val_loss = self.validate(val_dataloader)\n",
    "                self.val_losses.append(val_loss)\n",
    "                logger.info(f'Epoch {epoch+1}/{self.config.num_epochs} - Val Loss: {val_loss:.4f}')\n",
    "                \n",
    "                # Save best model\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    self.save_model(f'best_multilingual_embedding_model.pt')\n",
    "                    logger.info(f'New best model saved with val loss: {val_loss:.4f}')\n",
    "            \n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model('final_multilingual_embedding_model.pt')\n",
    "        logger.info('Training completed!')\n",
    "    \n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        save_dir = \"model_checkpoints\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        full_path = os.path.join(save_dir, path)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'global_step': self.global_step\n",
    "        }, full_path)\n",
    "        \n",
    "        logger.info(f'Model saved to {full_path}')\n",
    "    \n",
    "    def save_checkpoint(self, path: str):\n",
    "        \"\"\"Save training checkpoint\"\"\"\n",
    "        save_dir = \"model_checkpoints\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        full_path = os.path.join(save_dir, path)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'global_step': self.global_step\n",
    "        }, full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "269abb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualEmbeddingInference:\n",
    "    \"\"\"Inference class for multilingual embedding model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, config: Optional[EmbeddingConfig] = None):\n",
    "        # Setup device\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        with safe_globals({\"__main__.EmbeddingConfig\": EmbeddingConfig}):\n",
    "            checkpoint = torch.load(model_path, map_location=self.device, weights_only=False)\n",
    "        # Load checkpoint\n",
    "        # checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.config = config or checkpoint['config']\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.base_model)\n",
    "        \n",
    "        # Load model\n",
    "        self.model = MultilingualEmbeddingModel(self.config)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        logger.info(f\"Model loaded from {model_path}\")\n",
    "        logger.info(f\"Using device: {self.device}\")\n",
    "    \n",
    "    def encode(self, texts: Union[str, List[str]], batch_size: int = 32, \n",
    "               show_progress: bool = True, normalize: bool = True) -> np.ndarray:\n",
    "        \"\"\"Encode texts to embeddings\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        \n",
    "        all_embeddings = []\n",
    "        \n",
    "        iterator = range(0, len(texts), batch_size)\n",
    "        if show_progress:\n",
    "            iterator = tqdm(iterator, desc=\"Encoding\")\n",
    "        \n",
    "        for i in iterator:\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize\n",
    "            try:\n",
    "                encoded = self.tokenizer(\n",
    "                    batch_texts,\n",
    "                    max_length=self.config.max_length,\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                \n",
    "                # Move to device\n",
    "                input_ids = encoded['input_ids'].to(self.device)\n",
    "                attention_mask = encoded['attention_mask'].to(self.device)\n",
    "                \n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    embeddings = self.model(input_ids, attention_mask)\n",
    "                    all_embeddings.append(embeddings.cpu().numpy())\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error encoding batch: {e}\")\n",
    "                # Return zero embeddings for failed batch\n",
    "                zero_embeddings = np.zeros((len(batch_texts), self.config.embedding_dim))\n",
    "                all_embeddings.append(zero_embeddings)\n",
    "        \n",
    "        result = np.vstack(all_embeddings)\n",
    "        \n",
    "        if normalize:\n",
    "            result = result / (np.linalg.norm(result, axis=1, keepdims=True) + 1e-8)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Compute cosine similarity between two texts\"\"\"\n",
    "        embeddings = self.encode([text1, text2], show_progress=False)\n",
    "        return float(cosine_similarity([embeddings[0]], [embeddings[1]])[0][0])\n",
    "    \n",
    "    def find_most_similar(self, query: str, texts: List[str], top_k: int = 5) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Find most similar texts to query\"\"\"\n",
    "        query_embedding = self.encode([query], show_progress=False)\n",
    "        text_embeddings = self.encode(texts, show_progress=True)\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, text_embeddings)[0]\n",
    "        \n",
    "        # Get top k most similar\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((texts[idx], float(similarities[idx])))\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "67438389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_environment():\n",
    "    \"\"\"Setup environment for cross-platform compatibility\"\"\"\n",
    "    system = platform.system()\n",
    "    logger.info(f\"Running on {system}\")\n",
    "    \n",
    "    if system == \"Darwin\":  # macOS\n",
    "        logger.info(\"Mac detected - optimizing for Apple Silicon\")\n",
    "        if torch.backends.mps.is_available():\n",
    "            logger.info(\"MPS backend available\")\n",
    "        else:\n",
    "            logger.info(\"MPS backend not available, using CPU\")\n",
    "    elif system == \"Windows\":\n",
    "        logger.info(\"Windows detected\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"CUDA available: {torch.cuda.get_device_name()}\")\n",
    "        else:\n",
    "            logger.info(\"CUDA not available, using CPU\")\n",
    "    \n",
    "    # Set environment variables for better performance\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0c7b2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main training and inference function\"\"\"\n",
    "    # Setup environment\n",
    "    setup_environment()\n",
    "    \n",
    "    # Configuration\n",
    "    config = EmbeddingConfig(\n",
    "        base_model=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        embedding_dim=768,\n",
    "        max_length=512,\n",
    "        batch_size=8,  # Reduced for better compatibility\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        warmup_steps=500,\n",
    "        temperature=0.05,\n",
    "        margin=0.3,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_train_samples=20000,\n",
    "        max_val_samples=2000,\n",
    "        dataset_config_name = \"pair-class\"\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Starting multilingual embedding model training...\")\n",
    "    \n",
    "    # Initialize dataset processor\n",
    "    processor = MultilingualDatasetProcessor(config)\n",
    "    \n",
    "    # Download and process dataset\n",
    "    logger.info(\"Processing dataset...\")\n",
    "    if not processor.download_and_process_allnli():\n",
    "        logger.error(\"Failed to process dataset\")\n",
    "        return\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.base_model)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = MultilingualEmbeddingDataset(\n",
    "        os.path.join(processor.data_dir, 'train_data.json'),\n",
    "        tokenizer,\n",
    "        config.max_length\n",
    "    )\n",
    "    \n",
    "    val_dataset = MultilingualEmbeddingDataset(\n",
    "        os.path.join(processor.data_dir, 'val_data.json'),\n",
    "        tokenizer,\n",
    "        config.max_length\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Set to 0 for Mac compatibility\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MultilingualEmbeddingModel(config)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = MultilingualEmbeddingTrainer(model, config)\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train(train_dataloader, val_dataloader)\n",
    "    \n",
    "    # Test inference\n",
    "    logger.info(\"Testing inference...\")\n",
    "    try:\n",
    "        inference = MultilingualEmbeddingInference(\n",
    "            'model_checkpoints/best_multilingual_embedding_model.pt',\n",
    "            config\n",
    "        )\n",
    "        \n",
    "        # Test multilingual similarity\n",
    "        test_cases = [\n",
    "            (\"What is machine learning?\", \"Machine learning is a subset of AI\"),\n",
    "            (\"Was ist maschinelles Lernen?\", \"Machine learning is a subset of AI\"),  # German\n",
    "            (\"Qu'est-ce que l'apprentissage automatique?\", \"Machine learning is a subset of AI\"),  # French\n",
    "            (\"¿Qué es el aprendizaje automático?\", \"Machine learning is a subset of AI\"),  # Spanish\n",
    "        ]\n",
    "        \n",
    "        for query, text in test_cases:\n",
    "            similarity = inference.similarity(query, text)\n",
    "            logger.info(f\"Similarity between '{query}' and '{text}': {similarity:.4f}\")\n",
    "        \n",
    "        # Test embedding generation\n",
    "        test_texts = [\n",
    "            \"Hello world\",\n",
    "            \"Hola mundo\",\n",
    "            \"Bonjour le monde\",\n",
    "            \"Hallo Welt\",\n",
    "            \"Ciao mondo\"\n",
    "        ]\n",
    "        \n",
    "        embeddings = inference.encode(test_texts)\n",
    "        logger.info(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "        \n",
    "        # Test cross-lingual retrieval\n",
    "        corpus = [\n",
    "            \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
    "            \"Maschinelles Lernen ist eine Methode der Datenanalyse, die den Aufbau analytischer Modelle automatisiert.\",\n",
    "            \"L'apprentissage automatique est une méthode d'analyse de données qui automatise la construction de modèles analytiques.\",\n",
    "            \"El aprendizaje automático es un método de análisis de datos que automatiza la construcción de modelos analíticos.\",\n",
    "            \"I love cooking pasta with tomatoes.\",\n",
    "            \"Sports are great for physical fitness.\",\n",
    "            \"Music helps me relax after work.\"\n",
    "        ]\n",
    "        \n",
    "        # Query in German, should find ML-related sentences\n",
    "        german_query = \"Was ist maschinelles Lernen?\"\n",
    "        results = inference.find_most_similar(german_query, corpus, top_k=3)\n",
    "        \n",
    "        logger.info(f\"\\nTop 3 results for German query: '{german_query}'\")\n",
    "        for i, (text, score) in enumerate(results):\n",
    "            logger.info(f\"{i+1}. Score: {score:.4f} - Text: {text[:100]}...\")\n",
    "        \n",
    "        # Query in English, should find ML-related sentences\n",
    "        english_query = \"What is machine learning?\"\n",
    "        results = inference.find_most_similar(english_query, corpus, top_k=3)\n",
    "        \n",
    "        logger.info(f\"\\nTop 3 results for English query: '{english_query}'\")\n",
    "        for i, (text, score) in enumerate(results):\n",
    "            logger.info(f\"{i+1}. Score: {score:.4f} - Text: {text[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in inference testing: {e}\")\n",
    "        logger.info(\"You can still use the trained model for inference later.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "496ecf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 19:57:19,537 - INFO - Running on Darwin\n",
      "2025-07-11 19:57:19,538 - INFO - Mac detected - optimizing for Apple Silicon\n",
      "2025-07-11 19:57:19,539 - INFO - MPS backend available\n",
      "2025-07-11 19:57:19,542 - INFO - Starting multilingual embedding model training...\n",
      "2025-07-11 19:57:19,543 - INFO - Processing dataset...\n",
      "2025-07-11 19:57:19,543 - INFO - Loading AllNLI dataset...\n",
      "Generating train split: 100%|██████████| 942069/942069 [00:00<00:00, 6473324.54 examples/s]\n",
      "Generating dev split: 100%|██████████| 19657/19657 [00:00<00:00, 4884905.42 examples/s]\n",
      "Generating test split: 100%|██████████| 19656/19656 [00:00<00:00, 5464883.96 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: sentence-transformers/all-nli DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 942069\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 19657\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 19656\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train data: 100%|██████████| 20000/20000 [40:32<00:00,  8.22it/s] \n",
      "Processing validation data: 100%|██████████| 2000/2000 [00:21<00:00, 93.30it/s] \n",
      "2025-07-11 20:38:36,985 - INFO - Saved 6886 samples to embedding_data/train_data.json\n",
      "2025-07-11 20:38:36,988 - INFO - Saved 779 samples to embedding_data/val_data.json\n",
      "2025-07-11 20:38:36,988 - INFO - Processed 6886 training samples and 779 validation samples\n",
      "2025-07-11 20:38:38,194 - INFO - Loaded 6886 samples from embedding_data/train_data.json\n",
      "2025-07-11 20:38:38,196 - INFO - Loaded 779 samples from embedding_data/val_data.json\n",
      "2025-07-11 20:39:44,621 - INFO - Using MPS (Apple Silicon) device\n",
      "2025-07-11 20:39:45,712 - INFO - Starting training for 3 epochs...\n",
      "2025-07-11 20:39:45,713 - INFO - Total training steps: 645\n",
      "Epoch 1/3: 100%|██████████| 861/861 [31:46<00:00,  2.21s/it, loss=0.0946, avg_loss=0.0111] \n",
      "2025-07-11 21:11:32,376 - INFO - Epoch 1/3 - Train Loss: 0.0111\n",
      "Validating: 100%|██████████| 98/98 [01:03<00:00,  1.55it/s]\n",
      "2025-07-11 21:12:35,603 - INFO - Epoch 1/3 - Val Loss: 0.0136\n",
      "2025-07-11 21:12:40,345 - INFO - Model saved to model_checkpoints/best_multilingual_embedding_model.pt\n",
      "2025-07-11 21:12:40,345 - INFO - New best model saved with val loss: 0.0136\n",
      "Epoch 2/3: 100%|██████████| 861/861 [31:44<00:00,  2.21s/it, loss=0.0114, avg_loss=0.0105]\n",
      "2025-07-11 21:44:24,534 - INFO - Epoch 2/3 - Train Loss: 0.0105\n",
      "Validating: 100%|██████████| 98/98 [01:03<00:00,  1.54it/s]\n",
      "2025-07-11 21:45:28,232 - INFO - Epoch 2/3 - Val Loss: 0.0136\n",
      "2025-07-11 21:45:33,388 - INFO - Model saved to model_checkpoints/best_multilingual_embedding_model.pt\n",
      "2025-07-11 21:45:33,389 - INFO - New best model saved with val loss: 0.0136\n",
      "Epoch 3/3: 100%|██████████| 861/861 [32:47<00:00,  2.29s/it, loss=0.2051, avg_loss=0.0109]\n",
      "2025-07-11 22:18:21,368 - INFO - Epoch 3/3 - Train Loss: 0.0109\n",
      "Validating: 100%|██████████| 98/98 [01:01<00:00,  1.60it/s]\n",
      "2025-07-11 22:19:22,722 - INFO - Epoch 3/3 - Val Loss: 0.0136\n",
      "2025-07-11 22:19:27,317 - INFO - Model saved to model_checkpoints/best_multilingual_embedding_model.pt\n",
      "2025-07-11 22:19:27,318 - INFO - New best model saved with val loss: 0.0136\n",
      "2025-07-11 22:19:31,581 - INFO - Model saved to model_checkpoints/final_multilingual_embedding_model.pt\n",
      "2025-07-11 22:19:31,582 - INFO - Training completed!\n",
      "2025-07-11 22:19:31,583 - INFO - Testing inference...\n",
      "2025-07-11 22:19:34,045 - ERROR - Error in inference testing: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.EmbeddingConfig was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.EmbeddingConfig])` or the `torch.serialization.safe_globals([__main__.EmbeddingConfig])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "2025-07-11 22:19:34,051 - INFO - You can still use the trained model for inference later.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "caff9652",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = EmbeddingConfig(\n",
    "        base_model=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "        embedding_dim=768,\n",
    "        max_length=512,\n",
    "        batch_size=8,  # Reduced for better compatibility\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        warmup_steps=500,\n",
    "        temperature=0.05,\n",
    "        margin=0.3,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_train_samples=20000,\n",
    "        max_val_samples=2000,\n",
    "        dataset_config_name = \"pair-class\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d04affd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 22:38:50,953 - INFO - Model loaded from model_checkpoints/final_multilingual_embedding_model.pt\n",
      "2025-07-11 22:38:50,954 - INFO - Using device: mps\n"
     ]
    }
   ],
   "source": [
    "inference = MultilingualEmbeddingInference(\n",
    "            'model_checkpoints/final_multilingual_embedding_model.pt',\n",
    "            config\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cb324809",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]\n",
      "2025-07-11 22:59:35,653 - INFO - \n",
      "Top 3 results for English query: 'customer review section and vehicle rental interface for Europcar?'\n",
      "2025-07-11 22:59:35,653 - INFO - 1. Score: 0.8464 - Text: This page contains a customer review section and vehicle rental interface for Europcar Italy, featur...\n",
      "2025-07-11 22:59:35,653 - INFO - 2. Score: 0.7609 - Text: Europcar services matrix detailing premium mobility offerings and membership benefits, with emphasis...\n",
      "2025-07-11 22:59:35,653 - INFO - 3. Score: 0.7373 - Text: Comprehensive service matrix detailing Europcar's mobility solutions across six distinct categories,...\n"
     ]
    }
   ],
   "source": [
    "# Test multilingual similarity\n",
    "# test_cases = [\n",
    "#     (\"What is machine learning?\", \"Machine learning is a subset of AI\"),\n",
    "#     (\"i love dog\", \"i like cat\"),\n",
    "#     (\"Was ist maschinelles Lernen?\", \"Machine learning is a subset of AI\"),  # German\n",
    "#     (\"Qu'est-ce que l'apprentissage automatique?\", \"Machine learning is a subset of AI\"),  # French\n",
    "#     (\"¿Qué es el aprendizaje automático?\", \"Machine learning is a subset of AI\"),  # Spanish\n",
    "# ]\n",
    "\n",
    "# for query, text in test_cases:\n",
    "#     similarity = inference.similarity(query, text)\n",
    "#     logger.info(f\"Similarity between '{query}' and '{text}': {similarity:.4f}\")\n",
    "\n",
    "# # Test embedding generation\n",
    "# test_texts = [\n",
    "#     \"Hello world\",\n",
    "#     \"Hola mundo\",\n",
    "#     \"Bonjour le monde\",\n",
    "#     \"Hallo Welt\",\n",
    "#     \"Ciao mondo\"\n",
    "# ]\n",
    "\n",
    "# embeddings = inference.encode(test_texts)\n",
    "# logger.info(f\"Generated embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Test cross-lingual retrieval\n",
    "# corpus = [\n",
    "#     \"Machine learning is a method of data analysis that automates analytical model building.\",\n",
    "#     \"Maschinelles Lernen ist eine Methode der Datenanalyse, die den Aufbau analytischer Modelle automatisiert.\",\n",
    "#     \"L'apprentissage automatique est une méthode d'analyse de données qui automatise la construction de modèles analytiques.\",\n",
    "#     \"El aprendizaje automático es un método de análisis de datos que automatiza la construcción de modelos analíticos.\",\n",
    "#     \"I love cooking pasta with tomatoes.\",\n",
    "#     \"Sports are great for physical fitness.\",\n",
    "#     \"Music helps me relax after work.\"\n",
    "# ]\n",
    "corpus = ['This page from The EUROCALL Review (Volume 25, No. 2, September 2017) presents a data table analyzing mobile device usage for language learning among second-year B.A. students. The table captures key metrics including device types (predominantly smartphones with some tablet usage), duration of mobile learning experience (ranging 2-5 years), and self-assessed competency levels. The document includes both qualitative research questions and quantitative analysis methodology, with findings organized into usage patterns, study performance, and mobile learning encounters.\\n\\n',\n",
    " 'This image shows both a pictorial and schematic representation of a basic electrical circuit diagram, featuring key measurement components for financial analysis of electrical systems. The circuit documentation includes parallel voltage/current measurement capabilities using an ammeter (1-4A range) and voltmeter across a 3-ohm resistor powered by a 12V battery source. The dual diagram format enables cross-referencing between practical component layout and theoretical circuit analysis, supporting both operational and financial performance calculations.\\n\\n',\n",
    " 'This page contains a customer review section and vehicle rental interface for Europcar Italy, featuring a negative review about service issues in Florence alongside a cookie consent notice. The document includes a vehicle category selection panel displaying four rental segments: City car, Electric, Premium, and Van & Truck options, each with representative vehicle images and \"See more\" navigation links. A feature list section discusses MVP app development and user experience considerations, followed by information about name change fees and booking policies.\\n\\n',\n",
    " 'Europcar counter manual section detailing specialized pick-up procedures for Accor Bienvenue Card (I&O) rentals in Germany, version 06-2023, page 11/15. Document outlines identification requirements, reservation protocols, and payment procedures for rentals under €2,500.00, with specific focus on military procurement orders and tax-free rental processing. Contains detailed instructions for handling Bienvenue Card validation, rental agreement documentation, and proper form distribution to military units. Notable for its specialized procedures regarding military procurement thresholds and mandatory unit order requirements for rentals exceeding €2,500.00.\\n\\n',\n",
    " 'Rental policy document section detailing discount terms and restrictions for Greenway location, with specific focus on billing procedures and loyalty program exclusions. Contains procedural requirements for Location Manager approval of billing period modifications, including mandatory documentation in Rental Notes (F7) system. Notable for explicit exclusion of Lufthansa Miles&More program and prohibition on earning reward miles/points, while covering comprehensive fee structure including CDW, TW, and airport fees.\\n\\n',\n",
    " 'This academic research data table and analysis from The EUROCALL Review (Volume 25, No. 2, September 2017) presents mobile device usage patterns among language learners. The document contains a detailed participant matrix showing device types, experience levels, and usage duration, followed by qualitative analysis sections on mobile device adoption rationales and learning resource preferences. The findings highlight student utilization of online dictionaries (including diki and ColorDict), translation tools (Google Translate, Duolingo), and language learning apps (Fiszkoteka), with supplementary data on multimedia resource usage patterns through platforms like Vscreen, WhatsApp, TED and online newspapers.\\n\\n',\n",
    " \"Comprehensive service matrix detailing Europcar's mobility solutions across six distinct categories, with availability specifications ranging from worldwide to select locations. This tabular overview presents core rental offerings including standard vehicles, specialty transport, and sustainable options, with particular emphasis on their growing electric vehicle fleet and business-focused solutions. The document structure enables analysis of service coverage, market penetration, and operational capabilities across different business segments, supported by detailed service descriptions and geographic availability indicators.\\n\\n\",\n",
    " 'Europcar counter manual section detailing NATO troop vehicle rental procedures and documentation requirements. The document outlines specific pick-up protocols requiring written orders from NATO troops and business account verification through MOP BHPO checkout system. Contains detailed instructions for processing forms during vehicle return, including requirements for official service stamps and handling of multiple rentals on a single form, with annotated example showing key form fields for payment amount, payment method, and troop unit stamp placement.\\n\\n',\n",
    " 'Two variable DC power supply circuit diagrams are shown - a 0-30V/10A design using LM317 and dual display, and a simpler 0-30V/2A version with transformer input. The first circuit features digital voltage/current monitoring, dual 2N3055 power transistors, and potentiometer controls for voltage/current adjustment. The second diagram shows a basic transformer-based design using 1N4007 rectifiers, filter capacitors, and BC548 transistors for regulation. Both circuits provide adjustable DC output with voltage/current limiting capabilities, suitable for bench power supply applications requiring variable voltage control and overload protection.\\n\\n',\n",
    " \"Europcar workforce analysis document showing detailed gender distribution metrics across management and pay quartiles for 2020-2022. Management-level data presented via bar and pie charts demonstrates distribution ratios, while four separate pie charts break down gender composition across pay quartiles from lowest to highest paid segments. The document enables analysis of gender representation trends, pay equity patterns, and vertical segregation within the organization's hierarchy. Key metrics include management-level proportions and quartile-specific gender ratios, allowing for comprehensive diversity and inclusion assessment.\\n\\n\",\n",
    " 'Europcar manufacturer restrictions matrix document detailing vehicle-specific rental limitations for passenger cars (PKW) and light commercial vehicles (LKW). The table provides maximum mileage allowances, rental duration limits (ranging from 84-180 days), and minimum power requirements for various vehicle brands including premium (Audi, BMW), mainstream (VW, Toyota), and commercial vehicles. Comprehensive reference guide for rental agents showing manufacturer-mandated constraints across the full vehicle fleet, with separate sections for passenger and commercial vehicle restrictions including specific LFZ/LZC classifications.\\n\\n',\n",
    " 'Document formatting specifications page containing detailed layout and style guidelines for academic paper preparation. Key sections include Abstract formatting requirements (200-275 words in Times New Roman), Index Terms guidelines, and comprehensive Page Layout instructions with two-column format specifications. Contains precise measurements for margins, spacing, and indentation, along with detailed formatting rules for section headings, bullets, and references. Notable for its specific requirements around paper size (8.5\"x11\"), portrait orientation, and column width (3.4\") specifications. Includes both technical document structure requirements and stylistic conventions for academic publishing.\\n\\n',\n",
    " \"Europcar services matrix detailing premium mobility offerings and membership benefits, with emphasis on the Europcar Privilege loyalty program and corporate mobility solutions. Document includes comprehensive coverage of rental add-ons, from insurance protection to equipment upgrades, structured in a tabular format with three columns detailing service categories, descriptions, and availability/eligibility. Supplemental sections outline Europcar's sustainability initiatives focusing on electric vehicle fleet expansion and their customer experience framework highlighting 24/7 support and digital booking capabilities.\\n\\n\",\n",
    " 'This appears to be a quick start guide for a Flip video camera, featuring comprehensive setup and usage instructions across multiple sections. The document includes detailed diagrams showing device features, connectivity options, and operational controls, with clear step-by-step guidance for computer connection and TV viewing. Notable sections cover camcorder features, software capabilities, and recording/playback functions, presented in an accessible format with numbered steps and annotated illustrations. The guide effectively combines technical specifications with user-friendly visual aids to facilitate device operation and content sharing.',\n",
    " 'IEEE document formatting guidelines page showing detailed specifications for figures, tables, and equations. Contains a reference table of point sizes and type styles for different document elements, with entries ranging from 8 to 24 points. Features the IEEE logo and demonstrates proper equation formatting with a sample differential equation. Includes specific instructions for graphics resolution requirements (600 dpi monochrome, 300 dpi grayscale/color) and proper insertion methods. Document emphasizes formatting rules for headings, captions, and table text using Times New Roman font in various styles (UPPERCASE, Small Caps, Bold). Notable for its technical documentation structure and precise typographical specifications for academic/engineering publications.\\n\\n']\n",
    "\n",
    "# Query in German, should find ML-related sentences\n",
    "# german_query = \"Kundenbewertungsbereich und Schnittstelle zur Fahrzeugvermietung für Europcar\"\n",
    "# results = inference.find_most_similar(german_query, corpus, top_k=3)\n",
    "\n",
    "# logger.info(f\"\\nTop 3 results for German query: '{german_query}'\")\n",
    "# for i, (text, score) in enumerate(results):\n",
    "#     logger.info(f\"{i+1}. Score: {score:.4f} - Text: {text[:100]}...\")\n",
    "\n",
    "# Query in English, should find ML-related sentences\n",
    "english_query = \"customer review section and vehicle rental interface for Europcar?\"\n",
    "results = inference.find_most_similar(english_query, corpus, top_k=3)\n",
    "\n",
    "logger.info(f\"\\nTop 3 results for English query: '{english_query}'\")\n",
    "for i, (text, score) in enumerate(results):\n",
    "    logger.info(f\"{i+1}. Score: {score:.4f} - Text: {text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "70a02ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "final_data = []\n",
    "for i in dataaa['results']:\n",
    "    final_data.append(i['chunk_data'])\n",
    "\n",
    "# Randomly shuffle the list in place\n",
    "random.shuffle(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2b034833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This page from The EUROCALL Review (Volume 25, No. 2, September 2017) presents a data table analyzing mobile device usage for language learning among second-year B.A. students. The table captures key metrics including device types (predominantly smartphones with some tablet usage), duration of mobile learning experience (ranging 2-5 years), and self-assessed competency levels. The document includes both qualitative research questions and quantitative analysis methodology, with findings organized into usage patterns, study performance, and mobile learning encounters.\\n\\n',\n",
       " 'This image shows both a pictorial and schematic representation of a basic electrical circuit diagram, featuring key measurement components for financial analysis of electrical systems. The circuit documentation includes parallel voltage/current measurement capabilities using an ammeter (1-4A range) and voltmeter across a 3-ohm resistor powered by a 12V battery source. The dual diagram format enables cross-referencing between practical component layout and theoretical circuit analysis, supporting both operational and financial performance calculations.\\n\\n',\n",
       " 'This page contains a customer review section and vehicle rental interface for Europcar Italy, featuring a negative review about service issues in Florence alongside a cookie consent notice. The document includes a vehicle category selection panel displaying four rental segments: City car, Electric, Premium, and Van & Truck options, each with representative vehicle images and \"See more\" navigation links. A feature list section discusses MVP app development and user experience considerations, followed by information about name change fees and booking policies.\\n\\n',\n",
       " 'Europcar counter manual section detailing specialized pick-up procedures for Accor Bienvenue Card (I&O) rentals in Germany, version 06-2023, page 11/15. Document outlines identification requirements, reservation protocols, and payment procedures for rentals under €2,500.00, with specific focus on military procurement orders and tax-free rental processing. Contains detailed instructions for handling Bienvenue Card validation, rental agreement documentation, and proper form distribution to military units. Notable for its specialized procedures regarding military procurement thresholds and mandatory unit order requirements for rentals exceeding €2,500.00.\\n\\n',\n",
       " 'Rental policy document section detailing discount terms and restrictions for Greenway location, with specific focus on billing procedures and loyalty program exclusions. Contains procedural requirements for Location Manager approval of billing period modifications, including mandatory documentation in Rental Notes (F7) system. Notable for explicit exclusion of Lufthansa Miles&More program and prohibition on earning reward miles/points, while covering comprehensive fee structure including CDW, TW, and airport fees.\\n\\n',\n",
       " 'This academic research data table and analysis from The EUROCALL Review (Volume 25, No. 2, September 2017) presents mobile device usage patterns among language learners. The document contains a detailed participant matrix showing device types, experience levels, and usage duration, followed by qualitative analysis sections on mobile device adoption rationales and learning resource preferences. The findings highlight student utilization of online dictionaries (including diki and ColorDict), translation tools (Google Translate, Duolingo), and language learning apps (Fiszkoteka), with supplementary data on multimedia resource usage patterns through platforms like Vscreen, WhatsApp, TED and online newspapers.\\n\\n',\n",
       " \"Comprehensive service matrix detailing Europcar's mobility solutions across six distinct categories, with availability specifications ranging from worldwide to select locations. This tabular overview presents core rental offerings including standard vehicles, specialty transport, and sustainable options, with particular emphasis on their growing electric vehicle fleet and business-focused solutions. The document structure enables analysis of service coverage, market penetration, and operational capabilities across different business segments, supported by detailed service descriptions and geographic availability indicators.\\n\\n\",\n",
       " 'Europcar counter manual section detailing NATO troop vehicle rental procedures and documentation requirements. The document outlines specific pick-up protocols requiring written orders from NATO troops and business account verification through MOP BHPO checkout system. Contains detailed instructions for processing forms during vehicle return, including requirements for official service stamps and handling of multiple rentals on a single form, with annotated example showing key form fields for payment amount, payment method, and troop unit stamp placement.\\n\\n',\n",
       " 'Two variable DC power supply circuit diagrams are shown - a 0-30V/10A design using LM317 and dual display, and a simpler 0-30V/2A version with transformer input. The first circuit features digital voltage/current monitoring, dual 2N3055 power transistors, and potentiometer controls for voltage/current adjustment. The second diagram shows a basic transformer-based design using 1N4007 rectifiers, filter capacitors, and BC548 transistors for regulation. Both circuits provide adjustable DC output with voltage/current limiting capabilities, suitable for bench power supply applications requiring variable voltage control and overload protection.\\n\\n',\n",
       " \"Europcar workforce analysis document showing detailed gender distribution metrics across management and pay quartiles for 2020-2022. Management-level data presented via bar and pie charts demonstrates distribution ratios, while four separate pie charts break down gender composition across pay quartiles from lowest to highest paid segments. The document enables analysis of gender representation trends, pay equity patterns, and vertical segregation within the organization's hierarchy. Key metrics include management-level proportions and quartile-specific gender ratios, allowing for comprehensive diversity and inclusion assessment.\\n\\n\",\n",
       " 'Europcar manufacturer restrictions matrix document detailing vehicle-specific rental limitations for passenger cars (PKW) and light commercial vehicles (LKW). The table provides maximum mileage allowances, rental duration limits (ranging from 84-180 days), and minimum power requirements for various vehicle brands including premium (Audi, BMW), mainstream (VW, Toyota), and commercial vehicles. Comprehensive reference guide for rental agents showing manufacturer-mandated constraints across the full vehicle fleet, with separate sections for passenger and commercial vehicle restrictions including specific LFZ/LZC classifications.\\n\\n',\n",
       " 'Document formatting specifications page containing detailed layout and style guidelines for academic paper preparation. Key sections include Abstract formatting requirements (200-275 words in Times New Roman), Index Terms guidelines, and comprehensive Page Layout instructions with two-column format specifications. Contains precise measurements for margins, spacing, and indentation, along with detailed formatting rules for section headings, bullets, and references. Notable for its specific requirements around paper size (8.5\"x11\"), portrait orientation, and column width (3.4\") specifications. Includes both technical document structure requirements and stylistic conventions for academic publishing.\\n\\n',\n",
       " \"Europcar services matrix detailing premium mobility offerings and membership benefits, with emphasis on the Europcar Privilege loyalty program and corporate mobility solutions. Document includes comprehensive coverage of rental add-ons, from insurance protection to equipment upgrades, structured in a tabular format with three columns detailing service categories, descriptions, and availability/eligibility. Supplemental sections outline Europcar's sustainability initiatives focusing on electric vehicle fleet expansion and their customer experience framework highlighting 24/7 support and digital booking capabilities.\\n\\n\",\n",
       " 'This appears to be a quick start guide for a Flip video camera, featuring comprehensive setup and usage instructions across multiple sections. The document includes detailed diagrams showing device features, connectivity options, and operational controls, with clear step-by-step guidance for computer connection and TV viewing. Notable sections cover camcorder features, software capabilities, and recording/playback functions, presented in an accessible format with numbered steps and annotated illustrations. The guide effectively combines technical specifications with user-friendly visual aids to facilitate device operation and content sharing.',\n",
       " 'IEEE document formatting guidelines page showing detailed specifications for figures, tables, and equations. Contains a reference table of point sizes and type styles for different document elements, with entries ranging from 8 to 24 points. Features the IEEE logo and demonstrates proper equation formatting with a sample differential equation. Includes specific instructions for graphics resolution requirements (600 dpi monochrome, 300 dpi grayscale/color) and proper insertion methods. Document emphasizes formatting rules for headings, captions, and table text using Times New Roman font in various styles (UPPERCASE, Small Caps, Bold). Notable for its technical documentation structure and precise typographical specifications for academic/engineering publications.\\n\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a7fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
